{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Municodes for all present municpals' building codes in the state of Georgia\n",
    "This notebook aims on scraping all the present building codes for all municipals in the state of Georgia and saving them as a txt file in the folder of their state and subfolder of their municipal. The Municodes website will be scraped in this notebook. You may try to use this notebook to scrape present codes for municipals of other states; however, depending on the Municode page format, the code may not work appropriately to scrape. This code was written specifically for the state of Georgia. Checkout the Readme file to learn morre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started\n",
    "<p>Make sure you have install all libraries before running any 'import\" codes.</p>\n",
    "<p>Set the base directory to the folder path location on your local device where you would like to store the state folder containing subfolders of the municipality-code-data.</p>\n",
    "\n",
    "<p>***Remember: If you change the URL to a different state, update state_name accordingly so that folder name is correct.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "\n",
    "url = \"https://library.municode.com/ga\"\n",
    "state_link = url\n",
    "\n",
    "# Directory where you want to store the folders\n",
    "base_directory = \"/Users/macarahmorgan/Guldi-Lab/Georgia_Municipals\"\n",
    "\n",
    "#Specify the state name associated with the URL\n",
    "state_name = \"Georgia\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the code below!\n",
    "\n",
    "The runtime is very long(days long) therefore it is recommended to run the code on a GPU of different server/device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Selenium to open the webpage\n",
    "driver = webdriver.Chrome()  # You'll need to download the appropriate WebDriver for your browser\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load completely (you might need to adjust the wait time)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Get the page source after JavaScript execution\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Parse the page source with BeautifulSoup\n",
    "soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "#defining text conversion function\n",
    "def extract_and_save_text(url, output_folder, file_name):\n",
    "    driver = None\n",
    "    try:\n",
    "        # Check if the output folder exists, if not, create it\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "        \n",
    "        # Concatenate .txt extension to the file name if it's not already there\n",
    "        file_name = file_name.replace(\" - \", \"-\")\n",
    "        file_name = file_name.replace(\" \", \"_\")\n",
    "        if not file_name.endswith('.txt'):\n",
    "            file_name += '.txt'\n",
    "        \n",
    "        # Limit the file path to the specified folder and file name\n",
    "        output_file_path = os.path.join(output_folder, file_name)\n",
    "        \n",
    "        # Check if the file already exists, if yes, don't append\n",
    "        if os.path.exists(output_file_path):\n",
    "            print(f\"File '{output_file_path}' already exists. Not appending the extracted text.\")\n",
    "            return\n",
    "        \n",
    "        # Use Selenium to open the webpage\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the page to load completely (you might need to adjust the wait time)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'chunks')))\n",
    "        \n",
    "        # Remove elements with class \"btn-group action-bar text-primary hidden-xs pull-right\" using JavaScript\n",
    "        script = \"\"\"\n",
    "        var elements = document.querySelectorAll('.btn-group.action-bar.text-primary.hidden-xs.pull-right');\n",
    "        elements.forEach(function(element) {\n",
    "            element.remove();\n",
    "        });\n",
    "        \"\"\"\n",
    "        driver.execute_script(script)\n",
    "        \n",
    "        # Find elements with class \"chunks\"\n",
    "        chunk_elements = driver.find_elements(By.CLASS_NAME, 'chunks')\n",
    "        \n",
    "        # Extract text from chunk elements\n",
    "        text_content = \"\"\n",
    "        for chunk in chunk_elements:\n",
    "            text_content += chunk.text.strip() + '\\n'\n",
    "        \n",
    "        # Save the extracted text to a text file\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            output_file.write(text_content)\n",
    "        \n",
    "        print(f\"Text extracted and saved to {output_file_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "    finally:\n",
    "        # Make sure to close the browser after extraction\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "\n",
    "old_link = ''\n",
    "def extract_sub_links(link, directory, heading_text):\n",
    "    global old_link\n",
    "    # Initialize the WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    try:\n",
    "        # Head to the provided link\n",
    "        driver.get(link)\n",
    "        \n",
    "        # Wait for <li> elements with nodedepth=\"2\"\n",
    "        wait = WebDriverWait(driver, 5)\n",
    "        click_load_more_button(driver)\n",
    "        if(link==old_link):\n",
    "            li_elements = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'li[nodedepth=\"3\"]')))\n",
    "        else:\n",
    "            li_elements = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'li[nodedepth=\"2\"]')))\n",
    "\n",
    "        # Traverse through <li> elements and extract heading text and href links from <a> tags\n",
    "        for li_element in li_elements:\n",
    "            a_element = li_element.find_element(By.TAG_NAME, \"a\")\n",
    "            head_text = a_element.text.strip()\n",
    "            href_link = a_element.get_attribute('href')\n",
    "            print(f\"Sub-Chapter: {head_text}\")\n",
    "            print(f\"Sub-Link: {href_link}\")\n",
    "            result_string = heading_text + \"-\" + head_text\n",
    "            old_link = href_link\n",
    "            extract_sub_links(href_link, directory, result_string)\n",
    "            print(\"---\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"No elements with nodedepth='2' found. Error: {str(e)}\")\n",
    "        \n",
    "        # Call the function to extract and save text to a file (you need to provide this function)\n",
    "        extract_and_save_text(link, directory, heading_text)\n",
    "        print(\"---\")\n",
    "\n",
    "def click_load_more_button(driver):\n",
    "    while True:\n",
    "        try:\n",
    "            # Wait for the button to be clickable\n",
    "            load_more_button = WebDriverWait(driver, 20).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Load more')]\"))\n",
    "            )\n",
    "            \n",
    "            if load_more_button:\n",
    "                # If the button is found, click it\n",
    "                load_more_button.click()\n",
    "                print(\"Clicked the 'Load more' button.\")\n",
    "        except TimeoutException:\n",
    "            # If the button is not found within the given time, print a message and break the loop\n",
    "            print(\"No more 'Load more' buttons found or not clickable.\")\n",
    "            break\n",
    "\n",
    "\n",
    "state_directory = os.path.join(base_directory, state_name)\n",
    "os.makedirs(state_directory, exist_ok=True)  # Create state folder if not exists\n",
    "\n",
    "# Visit the state page\n",
    "driver.get(state_link)\n",
    "    \n",
    "# Wait for the state page to load\n",
    "driver.implicitly_wait(10)\n",
    "    \n",
    "# Get the page source after JavaScript execution\n",
    "state_page_source = driver.page_source\n",
    "    \n",
    "# Parse the state page with BeautifulSoup\n",
    "state_soup = BeautifulSoup(state_page_source, \"html.parser\")\n",
    "try:\n",
    "    # Wait for the city elements to be present\n",
    "    city_elements = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_all_elements_located(\n",
    "            (By.CSS_SELECTOR, 'li[ng-repeat=\"client in letterGroup.clients\"].col-xs-12.col-sm-6.col-md-4.col-lg-3.text-center')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\"Found {len(city_elements)} city elements.\")\n",
    "    \n",
    "    # Now you can proceed with processing the city elements\n",
    "        # Extract city names and links for the current state\n",
    "    city_data = []\n",
    "    for city_element in city_elements:\n",
    "        try:\n",
    "            city_name = city_element.text.strip()\n",
    "            city_link = city_element.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "            city_data.append({\"name\": city_name, \"link\": city_link})\n",
    "        except StaleElementReferenceException:\n",
    "            print(\"Stale Element Exception. Retrying city elements retrieval...\")\n",
    "            continue\n",
    "\n",
    "    for city_info in city_data:\n",
    "        print(\"****************************************************************\")\n",
    "        city_name = city_info[\"name\"]\n",
    "        city_link = city_info[\"link\"]\n",
    "        try:\n",
    "            # Use city_link and city_name for further processing\n",
    "            print(f\"City Name: {city_name}\")\n",
    "            print(f\"City Link: {city_link}\")\n",
    "            \n",
    "            # Create city directory inside the state directory\n",
    "            city_directory = os.path.join(state_directory, city_name)\n",
    "            os.makedirs(city_directory, exist_ok=True)\n",
    "\n",
    "            if state_name:\n",
    "                driver.get(city_link) #visit the city page of ordinances\n",
    "                #scrape textual data of ordinances\n",
    "                #convert to txt file\n",
    "                #add to city directory folder\n",
    "                # Wait for the page to load completely (you might need to adjust the wait time)\n",
    "                driver.implicitly_wait(5)\n",
    "\n",
    "                try:\n",
    "                # Use explicit wait to wait for the elements to be present\n",
    "                    wait = WebDriverWait(driver, 5)\n",
    "                    \n",
    "                    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'li[ng-repeat=\"node in toc.topLevelNodes track by node.Id\"]')))\n",
    "                    \n",
    "                    # Get the page source after JavaScript execution\n",
    "                    page_source = driver.page_source\n",
    "\n",
    "                    # Parse the page source with BeautifulSoup\n",
    "                    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "                    click_load_more_button(driver)\n",
    "                    \n",
    "\n",
    "                    # Find <li> elements with ng-repeat=\"node in toc.topLevelNodes track by node.Id\"\n",
    "                    chapters = soup.find_all(\"li\", {\"ng-repeat\": \"node in toc.topLevelNodes track by node.Id\"})\n",
    "                \n",
    "                    # Traverse through <li> elements and extract href links from <a> tags\n",
    "                    for li_element in chapters:\n",
    "                        a_element = li_element.find(\"a\", class_=\"toc-item-heading\")\n",
    "                        if a_element:\n",
    "                            heading_text = a_element.text.strip()\n",
    "                            href_link = a_element[\"href\"]\n",
    "                            print(f\"Section Name: {heading_text}\")\n",
    "                            print(f\"Link: {href_link}\")\n",
    "                            print()\n",
    "                            click_load_more_button(driver)\n",
    "                            \n",
    "                            #while a link is returned call again\n",
    "                            extract_sub_links(href_link, city_directory, heading_text)\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred: {str(e)}\") #a \"browse\" button exists\n",
    "                    #click browse button and try again\n",
    "                    #<a class=\"btn btn-primary btn-raised\" aria-label=\"Browse Code of Ordinances\" href=\"/ga/alpharetta/codes/code_of_ordinances\" ng-href=\"/ga/alpharetta/codes/code_of_ordinances\"><span class=\"\">Browse</span> Â»</a>\n",
    "                    # Check if the \"Browse\" button exists\n",
    "                    browse_button = driver.find_element(By.CSS_SELECTOR, 'a.btn.btn-primary.btn-raised[aria-label=\"Browse Code of Ordinances\"]')\n",
    "                    #get new link and assign it to city_link\n",
    "                    if browse_button:\n",
    "                        browse_button.click()\n",
    "                        # Wait for the page to load completely\n",
    "                        try:\n",
    "                            wait = WebDriverWait(driver, 10)\n",
    "                            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'li[nodedepth=\"2\"]')))\n",
    "\n",
    "                            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'li[ng-repeat=\"node in toc.topLevelNodes track by node.Id\"]')))\n",
    "                    \n",
    "                            # Get the page source after JavaScript execution\n",
    "                            page_source = driver.page_source\n",
    "\n",
    "                            # Parse the page source with BeautifulSoup\n",
    "                            soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "                            click_load_more_button(driver)\n",
    "                            \n",
    "\n",
    "                            # Find <li> elements with ng-repeat=\"node in toc.topLevelNodes track by node.Id\"\n",
    "                            chapters = soup.find_all(\"li\", {\"ng-repeat\": \"node in toc.topLevelNodes track by node.Id\"})\n",
    "                        \n",
    "                            # Traverse through <li> elements and extract href links from <a> tags\n",
    "                            for li_element in chapters:\n",
    "                                a_element = li_element.find(\"a\", class_=\"toc-item-heading\")\n",
    "                                if a_element:\n",
    "                                    heading_text = a_element.text.strip()\n",
    "                                    href_link = a_element[\"href\"]\n",
    "                                    print(f\"Section Name: {heading_text}\")\n",
    "                                    print(f\"Link: {href_link}\")\n",
    "                                    print()\n",
    "                                    click_load_more_button(driver)\n",
    "                                    \n",
    "                                    #while a link is returned call again\n",
    "                                    extract_sub_links(href_link, city_directory, heading_text)\n",
    "                        except Exception as e:\n",
    "                            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'li[ng-repeat=\"node in toc.topLevelNodes track by node.Id\"]')))\n",
    "                    \n",
    "                            # Get the page source after JavaScript execution\n",
    "                            page_source = driver.page_source\n",
    "\n",
    "                            # Parse the page source with BeautifulSoup\n",
    "                            soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "                            click_load_more_button(driver)\n",
    "                            \n",
    "\n",
    "                            # Find <li> elements with ng-repeat=\"node in toc.topLevelNodes track by node.Id\"\n",
    "                            chapters = soup.find_all(\"li\", {\"ng-repeat\": \"node in toc.topLevelNodes track by node.Id\"})\n",
    "                        \n",
    "                            # Traverse through <li> elements and extract href links from <a> tags\n",
    "                            for li_element in chapters:\n",
    "                                a_element = li_element.find(\"a\", class_=\"toc-item-heading\")\n",
    "                                if a_element:\n",
    "                                    heading_text = a_element.text.strip()\n",
    "                                    href_link = a_element[\"href\"]\n",
    "                                    print(f\"Section Name: {heading_text}\")\n",
    "                                    print(f\"Link: {href_link}\")\n",
    "                                    print()\n",
    "                                    \n",
    "                                    #while a link is returned call again\n",
    "                                    #extract_sub_links(href_link, city_directory, heading_text)\n",
    "                            continue\n",
    "        except StaleElementReferenceException:\n",
    "            print(\"Stale Element Exception. Retrying...\")\n",
    "            continue\n",
    "    \n",
    "except TimeoutException as ex:\n",
    "    print(f\"Timed out waiting for city elements: {ex}\")\n",
    "                    \n",
    "\n",
    "            \n",
    "        \n",
    "    \n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

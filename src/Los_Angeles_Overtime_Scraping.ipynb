{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Municode Archive for Los Angeles Building Codes from 2012-2023\n",
    "This notebook aims on scraping all the archived building codes for the Atlanta municipals in the state of Georgia and saving them as txt files in the folder of their municipal and subfolder of their publish date. The Municode website will be scraped in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started\n",
    "Make sure you have install all libraries before running any 'import\" codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify which codes from archive\n",
    "Create an array of the URLs for the specified editions of the archive in array_list and, in the exact same order, add the correspondind dates to the \"names\" array in the same formatted exemplified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = [\n",
    "     \"https://library.municode.com/ca/los_angeles_county/codes/code_of_ordinances/189447\", #10-08-2012\n",
    "     \"https://library.municode.com/ca/los_angeles_county/codes/code_of_ordinances/208697\", #09-27-2013\n",
    "     \"https://library.municode.com/ca/los_angeles_county/codes/code_of_ordinances/229682\", #09-11-2014\n",
    "     \"https://library.municode.com/ca/los_angeles_county/codes/code_of_ordinances/253976\", #10-05-2015\n",
    "     \"https://library.municode.com/ca/los_angeles_county/codes/code_of_ordinances/278482\", #10-05-2016\n",
    "     \"https://library.municode.com/ca/los_angeles_county/codes/code_of_ordinances/304298\", #09-27-2017\n",
    "     \"https://library.municode.com/ca/los_angeles_county/codes/code_of_ordinances/328365\", #09-18-2018\n",
    "     \"https://library.municode.com/ca/los_angeles_county/codes/code_of_ordinances/349596\", #09-19-2019\n",
    "     \"https://library.municode.com/ca/los_angeles_county/codes/code_of_ordinances/370506\", #09-28-2020\n",
    "     \"https://library.municode.com/ca/los_angeles_county/codes/code_of_ordinances/391668\", #09-29-2021\n",
    "     \"https://library.municode.com/ca/los_angeles_county/codes/code_of_ordinances/410890\", #10-10-2022\n",
    "     \"https://library.municode.com/ca/los_angeles_county/codes/code_of_ordinances/418767\", #10-09-2023\n",
    "]\n",
    "\n",
    "names = [\"10-08-2012_Codes\", \"09-27-2013_Codes\", \"09-11-2014_Codes\", \"10-05-2015_Codes\", \"10-05-2016_Codes\", \"09-27-2017_Codes\", \"09-18-2018_Codes\", \"09-19-2019_Codes\", \"09-28-2020_Codes\", \"09-29-2021_Codes\", \"10-10-2022_Codes\", \"10-09-2023_Codes\"]\n",
    "# Use Selenium to open the webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Base Directory\n",
    "Set base_directory to the directory/folder in which you would like the files stored on your local device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = \"/Users/macarahmorgan/Guldi-Lab/Sample/LosAngeles_Overtime\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Code!\n",
    "The scraping code has a significant run time that can take anywhere betweem 12 hours and an indefinite amount of days depending on the volume of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()  # You'll need to download the appropriate WebDriver for your browser\n",
    "\n",
    "#defining text conversion function\n",
    "def extract_and_save_text(url, output_folder, file_name):\n",
    "    driver = None\n",
    "    try:\n",
    "        # Check if the output folder exists, if not, create it\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "        \n",
    "        # Concatenate .txt extension to the file name if it's not already there\n",
    "        file_name = file_name.replace(\" - \", \"-\")\n",
    "        file_name = file_name.replace(\" \", \"_\")\n",
    "        if not file_name.endswith('.txt'):\n",
    "            file_name += '.txt'\n",
    "        \n",
    "        # Limit the file path to the specified folder and file name\n",
    "        output_file_path = os.path.join(output_folder, file_name)\n",
    "        \n",
    "        # Check if the file already exists, if yes, don't append\n",
    "        if os.path.exists(output_file_path):\n",
    "            print(f\"File '{output_file_path}' already exists. Not appending the extracted text.\")\n",
    "            return\n",
    "        \n",
    "        # Use Selenium to open the webpage\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the page to load completely (you might need to adjust the wait time)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'chunks')))\n",
    "        \n",
    "        # Remove elements with class \"btn-group action-bar text-primary hidden-xs pull-right\" using JavaScript\n",
    "        script = \"\"\"\n",
    "        var elements = document.querySelectorAll('.btn-group.action-bar.text-primary.hidden-xs.pull-right');\n",
    "        elements.forEach(function(element) {\n",
    "            element.remove();\n",
    "        });\n",
    "        \"\"\"\n",
    "        driver.execute_script(script)\n",
    "        \n",
    "        # Find elements with class \"chunks\"\n",
    "        chunk_elements = driver.find_elements(By.CLASS_NAME, 'chunks')\n",
    "        \n",
    "        # Extract text from chunk elements\n",
    "        text_content = \"\"\n",
    "        for chunk in chunk_elements:\n",
    "            text_content += chunk.text.strip() + '\\n'\n",
    "        \n",
    "        # Save the extracted text to a text file\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            output_file.write(text_content)\n",
    "        \n",
    "        print(f\"Text extracted and saved to {output_file_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "    finally:\n",
    "        # Make sure to close the browser after extraction\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "\n",
    "old_link = ''\n",
    "def extract_sub_links(link, directory, heading_text):\n",
    "    global old_link\n",
    "    # Initialize the WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    try:\n",
    "        # Head to the provided link\n",
    "        driver.get(link)\n",
    "        \n",
    "        # Wait for <li> elements with nodedepth=\"2\"\n",
    "        wait = WebDriverWait(driver, 5)\n",
    "        click_load_more_button(driver)\n",
    "        if(link==old_link):\n",
    "            li_elements = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'li[nodedepth=\"3\"]')))\n",
    "        else:\n",
    "            li_elements = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'li[nodedepth=\"2\"]')))\n",
    "\n",
    "        # Traverse through <li> elements and extract heading text and href links from <a> tags\n",
    "        for li_element in li_elements:\n",
    "            a_element = li_element.find_element(By.TAG_NAME, \"a\")\n",
    "            head_text = a_element.text.strip()\n",
    "            href_link = a_element.get_attribute('href')\n",
    "            print(f\"Sub-Chapter: {head_text}\")\n",
    "            print(f\"Sub-Link: {href_link}\")\n",
    "            result_string = heading_text + \"-\" + head_text\n",
    "            old_link = href_link\n",
    "            extract_sub_links(href_link, directory, result_string)\n",
    "            print(\"---\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"No elements with nodedepth='2' found. Error: {str(e)}\")\n",
    "        \n",
    "        # Call the function to extract and save text to a file (you need to provide this function)\n",
    "        extract_and_save_text(link, directory, heading_text)\n",
    "        print(\"---\")\n",
    "\n",
    "def click_load_more_button(driver):\n",
    "    while True:\n",
    "        try:\n",
    "            # Wait for the button to be clickable\n",
    "            load_more_button = WebDriverWait(driver, 20).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Load more')]\"))\n",
    "            )\n",
    "            \n",
    "            if load_more_button:\n",
    "                # If the button is found, click it\n",
    "                load_more_button.click()\n",
    "                print(\"Clicked the 'Load more' button.\")\n",
    "        except TimeoutException:\n",
    "            # If the button is not found within the given time, print a message and break the loop\n",
    "            print(\"No more 'Load more' buttons found or not clickable.\")\n",
    "            break\n",
    "\n",
    "\n",
    "try:\n",
    "    for index, url in enumerate(url_list):\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the page to load completely (you might need to adjust the wait time)\n",
    "        driver.implicitly_wait(10)\n",
    "\n",
    "        # Get the page source after JavaScript execution\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Parse the page source with BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        city_link = url\n",
    "\n",
    "        # Find the parent container by class (if applicable)\n",
    "        parent_container = soup.find('div', class_='parent-container')\n",
    "\n",
    "        folder_name = names[index]\n",
    "\n",
    "        # Create city directory inside the state directory\n",
    "        year_directory = os.path.join(base_directory, folder_name)\n",
    "        os.makedirs(year_directory, exist_ok=True)\n",
    "\n",
    "\n",
    "        driver.get(city_link) \n",
    "\n",
    "        driver.implicitly_wait(5)\n",
    "\n",
    "        try:\n",
    "                    # Use explicit wait to wait for the elements to be present\n",
    "                        wait = WebDriverWait(driver, 5)\n",
    "                        \n",
    "                        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'li[ng-repeat=\"node in toc.topLevelNodes track by node.Id\"]')))\n",
    "                        \n",
    "                        # Get the page source after JavaScript execution\n",
    "                        page_source = driver.page_source\n",
    "\n",
    "                        # Parse the page source with BeautifulSoup\n",
    "                        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "                        click_load_more_button(driver)\n",
    "                        \n",
    "\n",
    "                        # Find <li> elements with ng-repeat=\"node in toc.topLevelNodes track by node.Id\"\n",
    "                        chapters = soup.find_all(\"li\", {\"ng-repeat\": \"node in toc.topLevelNodes track by node.Id\"})\n",
    "                    \n",
    "                        # Traverse through <li> elements and extract href links from <a> tags\n",
    "                        for li_element in chapters:\n",
    "                            a_element = li_element.find(\"a\", class_=\"toc-item-heading\")\n",
    "                            if a_element:\n",
    "                                heading_text = a_element.text.strip()\n",
    "                                href_link = a_element[\"href\"]\n",
    "                                print(f\"Section Name: {heading_text}\")\n",
    "                                print(f\"Link: {href_link}\")\n",
    "                                print()\n",
    "                                \n",
    "                                click_load_more_button(driver)\n",
    "                                #while a link is returned call again\n",
    "                                extract_sub_links(href_link, year_directory, heading_text)\n",
    "        except Exception as e:\n",
    "                        print(f\"An error occurred: {str(e)}\") #a \"browse\" button exists\n",
    "                        #click browse button and try again\n",
    "                        #<a class=\"btn btn-primary btn-raised\" aria-label=\"Browse Code of Ordinances\" href=\"/ga/alpharetta/codes/code_of_ordinances\" ng-href=\"/ga/alpharetta/codes/code_of_ordinances\"><span class=\"\">Browse</span> Â»</a>\n",
    "                        # Check if the \"Browse\" button exists\n",
    "                        browse_button = driver.find_element(By.CSS_SELECTOR, 'a.btn.btn-primary.btn-raised[aria-label=\"Browse Code of Ordinances\"]')\n",
    "                        #get new link and assign it to city_link\n",
    "                        if browse_button:\n",
    "                            browse_button.click()\n",
    "                            # Wait for the page to load completely\n",
    "                            try:\n",
    "                                wait = WebDriverWait(driver, 10)\n",
    "                                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'li[nodedepth=\"2\"]')))\n",
    "\n",
    "                                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'li[ng-repeat=\"node in toc.topLevelNodes track by node.Id\"]')))\n",
    "                        \n",
    "                                # Get the page source after JavaScript execution\n",
    "                                page_source = driver.page_source\n",
    "\n",
    "                                # Parse the page source with BeautifulSoup\n",
    "                                soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "                                \n",
    "\n",
    "                                # Find <li> elements with ng-repeat=\"node in toc.topLevelNodes track by node.Id\"\n",
    "                                chapters = soup.find_all(\"li\", {\"ng-repeat\": \"node in toc.topLevelNodes track by node.Id\"})\n",
    "                            \n",
    "                                # Traverse through <li> elements and extract href links from <a> tags\n",
    "                                for li_element in chapters:\n",
    "                                    a_element = li_element.find(\"a\", class_=\"toc-item-heading\")\n",
    "                                    if a_element:\n",
    "                                        heading_text = a_element.text.strip()\n",
    "                                        href_link = a_element[\"href\"]\n",
    "                                        print(f\"Section Name: {heading_text}\")\n",
    "                                        print(f\"Link: {href_link}\")\n",
    "                                        print()\n",
    "                                        \n",
    "                                        click_load_more_button(driver)\n",
    "                                        #while a link is returned call again\n",
    "                                        extract_sub_links(href_link, year_directory, heading_text)\n",
    "                            except Exception as e:\n",
    "                                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'li[ng-repeat=\"node in toc.topLevelNodes track by node.Id\"]')))\n",
    "                        \n",
    "                                # Get the page source after JavaScript execution\n",
    "                                page_source = driver.page_source\n",
    "\n",
    "                                # Parse the page source with BeautifulSoup\n",
    "                                soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "                                click_load_more_button(driver)\n",
    "                                \n",
    "\n",
    "                                # Find <li> elements with ng-repeat=\"node in toc.topLevelNodes track by node.Id\"\n",
    "                                chapters = soup.find_all(\"li\", {\"ng-repeat\": \"node in toc.topLevelNodes track by node.Id\"})\n",
    "                            \n",
    "                                # Traverse through <li> elements and extract href links from <a> tags\n",
    "                                for li_element in chapters:\n",
    "                                    a_element = li_element.find(\"a\", class_=\"toc-item-heading\")\n",
    "                                    if a_element:\n",
    "                                        heading_text = a_element.text.strip()\n",
    "                                        href_link = a_element[\"href\"]\n",
    "                                        print(f\"Section Name: {heading_text}\")\n",
    "                                        print(f\"Link: {href_link}\")\n",
    "                                        print()\n",
    "                                        \n",
    "                                        click_load_more_button(driver)\n",
    "                                        #while a link is returned call again\n",
    "                                        extract_sub_links(href_link, year_directory, heading_text)\n",
    "\n",
    "finally:\n",
    "    # Close the browser after extraction\n",
    "    driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Municodes for all municpals in the state of Georgia\n",
    "This notebook aims on scraping all the present building codes for all municipals in the state of Georgia and saving them as a txt file in the folder of their state and subfolder of their municipal. The Municodes website will be scraped in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started\n",
    "Make sure you have install all libraries before running any 'import\" codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head to webpage of state directory on initial load\n",
    "Run all in order for code to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this url links to directory of all states in the Municode database\n",
    "url = \"https://library.municode.com/#G\"\n",
    "\n",
    "# Use Selenium to open the webpage\n",
    "driver = webdriver.Chrome()  # You'll need to download the appropriate WebDriver for your browser\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load completely (you might need to adjust the wait time)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Get the page source after JavaScript execution\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Parse the page source with BeautifulSoup\n",
    "soup = BeautifulSoup(page_source, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the txt file output function: extract_and_save_text\n",
    "This function appends the extracted text content to a txt file given the url, output folder name and the filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining text conversion function\n",
    "def extract_and_save_text(url, output_folder, file_name):\n",
    "    driver = None\n",
    "    try:\n",
    "        # Check if the output folder exists, if not, create it\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "        \n",
    "        # Concatenate .txt extension to the file name if it's not already there\n",
    "        file_name = file_name.replace(\" - \", \"-\")\n",
    "        file_name = file_name.replace(\" \", \"_\")\n",
    "        if not file_name.endswith('.txt'):\n",
    "            file_name += '.txt'\n",
    "        \n",
    "        # Limit the file path to the specified folder and file name\n",
    "        output_file_path = os.path.join(output_folder, file_name)\n",
    "        \n",
    "        # Check if the file already exists, if yes, don't append\n",
    "        if os.path.exists(output_file_path):\n",
    "            print(f\"File '{output_file_path}' already exists. Not appending the extracted text.\")\n",
    "            return\n",
    "        \n",
    "        # Use Selenium to open the webpage\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the page to load completely (you might need to adjust the wait time)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'chunks')))\n",
    "        \n",
    "        # Remove elements with class \"btn-group action-bar text-primary hidden-xs pull-right\" using JavaScript\n",
    "        script = \"\"\"\n",
    "        var elements = document.querySelectorAll('.btn-group.action-bar.text-primary.hidden-xs.pull-right');\n",
    "        elements.forEach(function(element) {\n",
    "            element.remove();\n",
    "        });\n",
    "        \"\"\"\n",
    "        driver.execute_script(script)\n",
    "        \n",
    "        # Find elements with class \"chunks\"\n",
    "        chunk_elements = driver.find_elements(By.CLASS_NAME, 'chunks')\n",
    "        \n",
    "        # Extract text from chunk elements\n",
    "        text_content = \"\"\n",
    "        for chunk in chunk_elements:\n",
    "            text_content += chunk.text.strip() + '\\n'\n",
    "        \n",
    "        # Save the extracted text to a text file\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            output_file.write(text_content)\n",
    "        \n",
    "        print(f\"Text extracted and saved to {output_file_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "    finally:\n",
    "        # Make sure to close the browser after extraction\n",
    "        if driver:\n",
    "            driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the function to extract the sublinks from the webpage\n",
    "If a link goes to another directory page filled with sublinks, extraxt the sublink urls and head to their pages to extract and save that text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_link = ''\n",
    "def extract_sub_links(link, directory, heading_text):\n",
    "    global old_link\n",
    "    # Initialize the WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    try:\n",
    "        # Head to the provided link\n",
    "        driver.get(link)\n",
    "        \n",
    "        # Wait for <li> elements with nodedepth=\"2\"\n",
    "        wait = WebDriverWait(driver, 5)\n",
    "        if(link==old_link):\n",
    "            li_elements = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'li[nodedepth=\"3\"]')))\n",
    "        else:\n",
    "            li_elements = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'li[nodedepth=\"2\"]')))\n",
    "\n",
    "        # Traverse through <li> elements and extract heading text and href links from <a> tags\n",
    "        for li_element in li_elements:\n",
    "            a_element = li_element.find_element(By.TAG_NAME, \"a\")\n",
    "            head_text = a_element.text.strip()\n",
    "            href_link = a_element.get_attribute('href')\n",
    "            print(f\"Sub-Chapter: {head_text}\")\n",
    "            print(f\"Sub-Link: {href_link}\")\n",
    "            result_string = heading_text + \"-\" + head_text\n",
    "            old_link = href_link\n",
    "            extract_sub_links(href_link, directory, result_string)\n",
    "            print(\"---\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"No elements with nodedepth='2' found. Error: {str(e)}\")\n",
    "        \n",
    "        # Call the function to extract and save text to a file (you need to provide this function)\n",
    "        extract_and_save_text(link, directory, heading_text)\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping through Municodes states and municipals\n",
    " ***Remember to set the variable \"base_directory\" to the directory in which you would like the files stored on your local device or else the code will not work. </p>\n",
    "\n",
    " Current code is specific to the state of Georgia due to the if statement: if state_name == \"Georgia\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/macarahmorgan/Guldi-Lab/Scrapping Code\n"
     ]
    }
   ],
   "source": [
    "# Find state links\n",
    "state_links_elements = soup.find_all(\"li\", {\"ng-repeat\": \"state in stateGroup.states\"}, class_=\"col-xs-12 col-md-6 col-lg-4 text-center\")\n",
    "\n",
    "# Extract state links and names\n",
    "state_data = []\n",
    "for state_element in state_links_elements:\n",
    "    state_name = state_element.text.strip()\n",
    "    state_link = state_element.find(\"a\")[\"href\"]\n",
    "    state_data.append({\"name\": state_name, \"link\": state_link})\n",
    "\n",
    "# Directory where you want to store the folders\n",
    "print(os.getcwd())\n",
    "base_directory = \"/Users/macarahmorgan/Guldi-Lab/Municode-Data\"\n",
    "\n",
    "\n",
    "# Iterate through state links and names\n",
    "for state in state_data:\n",
    "    state_link = state[\"link\"]\n",
    "    state_name = state[\"name\"]\n",
    "    state_directory = os.path.join(base_directory, state_name)\n",
    "    os.makedirs(state_directory, exist_ok=True)  # Create state folder if not exists\n",
    "\n",
    "    # Visit the state page\n",
    "    driver.get(state_link)\n",
    "    \n",
    "    # Wait for the state page to load\n",
    "    driver.implicitly_wait(10)\n",
    "    \n",
    "    # Get the page source after JavaScript execution\n",
    "    state_page_source = driver.page_source\n",
    "    \n",
    "    # Parse the state page with BeautifulSoup\n",
    "    state_soup = BeautifulSoup(state_page_source, \"html.parser\")\n",
    "    \n",
    "    # Find city elements for the current state\n",
    "    city_elements = state_soup.find_all(\"li\", {\"ng-repeat\": \"client in letterGroup.clients\"}, class_=\"col-xs-12 col-sm-6 col-md-4 col-lg-3 text-center\")\n",
    "    \n",
    "    # Extract city names and links for the current state\n",
    "    city_data = []\n",
    "    for city_element in city_elements:\n",
    "        city_name = city_element.text.strip()\n",
    "        city_link = city_element.find(\"a\")[\"href\"]\n",
    "        city_data.append({\"name\": city_name, \"link\": city_link})\n",
    "        # Create city directory inside the state directory\n",
    "        city_directory = os.path.join(state_directory, city_name)\n",
    "        os.makedirs(city_directory, exist_ok=True)\n",
    "\n",
    "        #we only want ordinances from Georgia cities\n",
    "        if state_name == \"Georgia\":\n",
    "            driver.get(city_link) #visit the city page of ordinances\n",
    "            #scrape textual data of ordinances\n",
    "            #convert to txt file\n",
    "            #add to city directory folder\n",
    "            # Wait for the page to load completely (you might need to adjust the wait time)\n",
    "            driver.implicitly_wait(5)\n",
    "\n",
    "            try:\n",
    "            # Use explicit wait to wait for the elements to be present\n",
    "                wait = WebDriverWait(driver, 5)\n",
    "                \n",
    "                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'li[ng-repeat=\"node in toc.topLevelNodes track by node.Id\"]')))\n",
    "                \n",
    "                # Get the page source after JavaScript execution\n",
    "                page_source = driver.page_source\n",
    "\n",
    "                # Parse the page source with BeautifulSoup\n",
    "                soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "                \n",
    "\n",
    "                # Find <li> elements with ng-repeat=\"node in toc.topLevelNodes track by node.Id\"\n",
    "                chapters = soup.find_all(\"li\", {\"ng-repeat\": \"node in toc.topLevelNodes track by node.Id\"})\n",
    "            \n",
    "                # Traverse through <li> elements and extract href links from <a> tags\n",
    "                for li_element in chapters:\n",
    "                    a_element = li_element.find(\"a\", class_=\"toc-item-heading\")\n",
    "                    if a_element:\n",
    "                        heading_text = a_element.text.strip()\n",
    "                        href_link = a_element[\"href\"]\n",
    "                        print(f\"Section Name: {heading_text}\")\n",
    "                        print(f\"Link: {href_link}\")\n",
    "                        print()\n",
    "                        \n",
    "                        #while a link is returned call again\n",
    "                        extract_sub_links(href_link, city_directory, heading_text)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {str(e)}\") #a \"browse\" button exists\n",
    "                #click browse button and try again\n",
    "                #<a class=\"btn btn-primary btn-raised\" aria-label=\"Browse Code of Ordinances\" href=\"/ga/alpharetta/codes/code_of_ordinances\" ng-href=\"/ga/alpharetta/codes/code_of_ordinances\"><span class=\"\">Browse</span> Â»</a>\n",
    "                # Check if the \"Browse\" button exists\n",
    "                browse_button = driver.find_element(By.CSS_SELECTOR, 'a.btn.btn-primary.btn-raised[aria-label=\"Browse Code of Ordinances\"]')\n",
    "                #get new link and assign it to city_link\n",
    "                if browse_button:\n",
    "                    browse_button.click()\n",
    "                    # Wait for the page to load completely\n",
    "                    try:\n",
    "                        wait = WebDriverWait(driver, 10)\n",
    "                        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'li[nodedepth=\"2\"]')))\n",
    "\n",
    "                        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'li[ng-repeat=\"node in toc.topLevelNodes track by node.Id\"]')))\n",
    "                \n",
    "                        # Get the page source after JavaScript execution\n",
    "                        page_source = driver.page_source\n",
    "\n",
    "                        # Parse the page source with BeautifulSoup\n",
    "                        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "                        \n",
    "\n",
    "                        # Find <li> elements with ng-repeat=\"node in toc.topLevelNodes track by node.Id\"\n",
    "                        chapters = soup.find_all(\"li\", {\"ng-repeat\": \"node in toc.topLevelNodes track by node.Id\"})\n",
    "                    \n",
    "                        # Traverse through <li> elements and extract href links from <a> tags\n",
    "                        for li_element in chapters:\n",
    "                            a_element = li_element.find(\"a\", class_=\"toc-item-heading\")\n",
    "                            if a_element:\n",
    "                                heading_text = a_element.text.strip()\n",
    "                                href_link = a_element[\"href\"]\n",
    "                                print(f\"Section Name: {heading_text}\")\n",
    "                                print(f\"Link: {href_link}\")\n",
    "                                print()\n",
    "                                \n",
    "                                #while a link is returned call again\n",
    "                                extract_sub_links(href_link, city_directory, heading_text)\n",
    "                    except Exception as e:\n",
    "                        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'li[ng-repeat=\"node in toc.topLevelNodes track by node.Id\"]')))\n",
    "                \n",
    "                        # Get the page source after JavaScript execution\n",
    "                        page_source = driver.page_source\n",
    "\n",
    "                        # Parse the page source with BeautifulSoup\n",
    "                        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "                        \n",
    "\n",
    "                        # Find <li> elements with ng-repeat=\"node in toc.topLevelNodes track by node.Id\"\n",
    "                        chapters = soup.find_all(\"li\", {\"ng-repeat\": \"node in toc.topLevelNodes track by node.Id\"})\n",
    "                    \n",
    "                        # Traverse through <li> elements and extract href links from <a> tags\n",
    "                        for li_element in chapters:\n",
    "                            a_element = li_element.find(\"a\", class_=\"toc-item-heading\")\n",
    "                            if a_element:\n",
    "                                heading_text = a_element.text.strip()\n",
    "                                href_link = a_element[\"href\"]\n",
    "                                print(f\"Section Name: {heading_text}\")\n",
    "                                print(f\"Link: {href_link}\")\n",
    "                                print()\n",
    "                                \n",
    "                                #while a link is returned call again\n",
    "                                extract_sub_links(href_link, city_directory, heading_text)\n",
    "                        continue\n",
    "                    \n",
    "\n",
    "            \n",
    "        \n",
    "    \n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
